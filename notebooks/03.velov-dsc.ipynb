{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lyon2 geonum](https://perso.liris.cnrs.fr/lmoncla/GEONUM/fig/logos.png)\n",
    "\n",
    "# 2A3 – Modélisation et structuration de données géographiques et applications SGBD spatiaux\n",
    "\n",
    "## Tutoriel : Analyse des données des disponibilités des stations Vélo'v de la Métropole de Lyon\n",
    "\n",
    "# Partie 3 : Un peu de science des données\n",
    "\n",
    "\n",
    "L'objectif de cette partie du tutoriel est de continuer l'analyse des données velo'v et d'expérimenter des méthodes d'intelligence artificielle pour le clustering des données mais également d'ajouter une couche de données météo pour faire de la prédiction des disponibilités vélo'v.\n",
    "\n",
    "Les objectifs de cette partie sont les suivants : \n",
    "\n",
    "* Entraîner un modèle d'apprentissage non supervisée (clustering) pour l'analyse des données.\n",
    "* Récupérer le jeu de données météo et l'associer à celui des velo'v.\n",
    "* Visualiser les données selon cette nouvelle couche météo.\n",
    "* Entraîner un modèle de prédiction de la demande des vélo'v.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configurer l'environnement\n",
    "\n",
    "### 1.2 Télécharger les fichiers et installer les librairies (seulement pour Google Colab)\n",
    "\n",
    "* Si vous avez déjà configuré votre environnement, soit avec conda, soit avec pip (voir le fichier [README.md](https://gitlab.liris.cnrs.fr/lmoncla/tutoriel-anf-tdm-2022-python-geoparsing/-/blob/main/README.md)), vous pouvez ignorer la section suivante et passer directement à la section 1.2.\n",
    "* Si vous exécutez ce notebook depuis Google Colab, vous devez exécuter les cellules suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/ludovicmoncla/master-geonum-tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r master-geonum-tutorials/requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importer les librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import folium\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import geopandas\n",
    "\n",
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Récupération des jeux de données\n",
    "\n",
    "Comme pour la Partie 1, l'ensemble des données utilisées dans ce tutoriel est disponible à cette adresse : \n",
    "https://perso.liris.cnrs.fr/lmoncla/GEONUM/\n",
    "\n",
    "* Télécharger les fichiers contenant les données\n",
    "1. data-stations.zip\n",
    "2. data-bikes-2.zip\n",
    "3. data-weather-lyon.csv\n",
    "\n",
    "Les 2 zip contiennent chacun un fichier CSV contenant respectivement la liste des stations vélov (et leur localisation) et la liste des disponibilités de chaque station par tranche de 30 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On commence par créer un dossier dans lequel on va télécharger les données\n",
    "## Peut être fait directement depuis l'explorateur de fichiers\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on se déplace dans le nouveau dossier\n",
    "%cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On télécharge l'archive contenant la liste des stations\n",
    "!wget https://perso.liris.cnrs.fr/lmoncla/GEONUM/data-stations.zip\n",
    "    \n",
    "## On télécharge l'archive contenant la liste des disponibilité des stations par tranche de 5 minutes\n",
    "!wget https://perso.liris.cnrs.fr/lmoncla/GEONUM/data-bikes-2.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Chargement des données\n",
    "\n",
    "Nous chargeons les mêmes jeux de données que dans le tutoriel précédent :\n",
    "\n",
    "1. les stations vélo'v (id station, latitude, longitude),\n",
    "2. leurs historiques (id station, année, mois, jour, heure, minute, date, vélos disponibles, places disponibles).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## On charge les données des stations dans un dataframe\n",
    "df_stations = *****\n",
    "\n",
    "## On crée maintenant le dataframe avec les données d'historique\n",
    "df_bikes = *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On affiche les premières lignes\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de la taille en mémoire\n",
    "\n",
    "## on transforme le type des colonnes en entier ou float lorsque cela est nécessaire\n",
    "df_bikes['time'] = pd.to_datetime(df_bikes['time']) \n",
    "df_bikes[['year', 'daily_departure', 'daily_arrival']] = df_bikes[['year', 'daily_departure', 'daily_arrival']].astype('int16')\n",
    "df_bikes[['month','day','hour','minute', 'bikes', 'bike_stands', 'departure30min','arrival30min']] = df_bikes[['month','day','hour','minute', 'bikes', 'bike_stands', 'departure30min','arrival30min']].astype('int8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "\n",
    "Notre objectif dans cette partie va être d'identifier des groupes de stations \"similaires\". Pour cela, nous allons appliquer des méthodes d'apprentissage non supervisées : clustering.\n",
    "L'obectif n'est pas de regrouper les stations par proximité spatial mais par une similarité calculée à partir des données d'historique de l'utilisation des stations.\n",
    "\n",
    "### 3.1 Préparation des données\n",
    "\n",
    "Dans un premier temps nous devons nous assurer que toutes les stations sont comparables. On s'intèresse donc à savoir si elles ont toutes le même nombre de données. Le manque de données peut provenir de bugs à différents niveaux du processus de mise à disposition des données mais cela peut également être dû à des stations qui auraient fermés en cours d'année.\n",
    "\n",
    "Afin de vérifier si toutes les stations ont le même nombre de données, nous pouvons regrouper les lignes du dataframe en fonction du nom de la station puis afficher la taille de chaque groupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# On regroupe les lignes du dataframe par station\n",
    "g = *****\n",
    "\n",
    "# On affiche la taille de chaque groupe\n",
    "print(g.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les stations ne s'affichent pas mais on peut déjà observer qu'une station à moins de données que les autres. Nous devons déterminer combien de stations sont différentes pour les supprimer.\n",
    "Utiliser la fonction [plot()](https://www.geeksforgeeks.org/plot-the-size-of-each-group-in-a-groupby-object-in-pandas/) pour afficher un graphique de la taille (nombre de données) de chaque station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche les tailles de chaque groupe sur un graphique\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que 6 stations ont nettement moins de données que les autres. Pour le reste des traitements nous allons donc supprimer ces 6 stations du jeu de données.\n",
    "\n",
    "Identifier le nom de ces 6 stations en utilisant les fonctions count() et [nsmallest()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nsmallest.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche la liste des 6 stations ayant le moins de données\n",
    "list_to_drop = *****\n",
    "list_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie la taille du dataframe avant la suppression des lignes des stations concernées\n",
    "df_bikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les 6 stations du dataframe df_bikes\n",
    "\n",
    "df_bikes.drop(*****].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie la taille du dataframe après la suppression\n",
    "df_bikes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de pouvoir regrouper les stations par similarité, nous allons ajouter quelques variables (colonnes). En particulier, nous allons nous intéresser au nombre de départs (et d'arrivées) normalisé en fonction du jour de la semaine. \n",
    "\n",
    "\n",
    "- Créer une nouvelle colonne `day_of_week` en utilisant la méthode [day_name()](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.day_name.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crée une nouvelle colonne avec le nom du jour de la semaine\n",
    "df_bikes['day_of_week'] = df_bikes['time'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite maintenant ajouter des colonnes avec les valeurs journalière des départs et arrivées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule les moyennes des arrivées et des départs (journalier) par station et par jour de la semaine\n",
    "arrivals = *****\n",
    "departures = *****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrivals = arrivals.unstack(level=1) # transforme les lignes en colonnes\n",
    "arrivals = arrivals.fillna(0) # on remplace les valeurs vide null\n",
    "arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departures = departures.unstack(level=1) # transforme les lignes en colonnes\n",
    "departures = departures.fillna(0) # on remplace les valeurs null\n",
    "departures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On combine ces deux jeux de données en un seul qui servira de jeu d'entrainement pour l'algorithme de clustering\n",
    "\n",
    "df_data = departures.merge(arrivals, how='inner', on=['id_velov'])\n",
    "df_data = df_data.fillna(0)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie s'il existe des valeurs infinies (non compatible avec l'algo de clustering)\n",
    "np.any(np.isfinite(df_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On remplace ces valeur par la valeur 1\n",
    "df_data.replace([np.inf, -np.inf], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entraînement et utilisation du modèle de clustering\n",
    "\n",
    "\n",
    "\n",
    "Les méthodes d'apprentissage automatique sont déjà implémentées au sein de la librairie [Scikit-Learn](https://scikit-learn.org/stable/). Cette librairie regroupe un grand nombre d'algorithmes d'apprentissage non supervisée (clustering) et supervisée (classification et regression).\n",
    "\n",
    "La méthode des [k-moyennes](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) ou Kmeans est un algorithme de clustering très utilisé. Son principe est simple regrouper les données en k clusters homogènes et compacts. Afin de créer des clusters homogènes l'algorithme se base sur un calcul de distance entre les données et le centroid des différents clusters. Ces centroids étant recalculer à chaque fois qu'une nouvelle données est ajoutée au cluster. \n",
    "\n",
    "Notre objectif ici, est de déterminer si les stations peuvent être regroupées en 2 clusters distincts en se basant sur la similarité de leur historique d'utilisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# déclaration du modèle\n",
    "model = *****\n",
    "\n",
    "# entrainement du modèle\n",
    "*****\n",
    "\n",
    "# utilisation du modèle pour associer un numéro de cluster à chaque ligne du jeu de données\n",
    "df_data[\"cluster\"] = *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir afficher ces clusters sur une carte il faut ajouter les coordonées latitude/longitude des stations à notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = *****\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Représentation cartographique des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On transforme le dataframe en geodataframe\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On affiche directement les données du geodataframe sur une carte \n",
    "## avec la méthode scatter_mapbox() de la librairie plotly.express:\n",
    "fig = *****\n",
    "\n",
    "## On supprime les marges autour de la carte\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.update(layout_coloraxis_showscale=False) # supprime la colorbar\n",
    "\n",
    "## On affiche la carte\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que les 2 clusters sont également distincts géographiquement. On remarque un cluster situé dans le centre et le deuxième situé plutôt en périphérie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ajout d'une couche de données supplémentaire : la météo\n",
    "\n",
    "\n",
    "### 4.1 Chargement des données\n",
    "\n",
    "\n",
    "Le fichier `data-weather-lyon.csv` contient les données météo par heure pour l'année 2021 pour Lyon.\n",
    "\n",
    "Le jeu de données météo correspond au dataset `NASA/POWER CERES/MERRA2 Native Resolution Hourly Data` récupéré grâce à l'API du site POWER Project de la NASA : https://power.larc.nasa.gov/.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On télécharge l'archive contenant les données météo\n",
    "!wget https://perso.liris.cnrs.fr/lmoncla/GEONUM/data-weather-lyon.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On charge les données météo dans un dataframe\n",
    "df_weather = pd.read_csv('data-weather-lyon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On affiche les premières lignes\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Premier apercu des données météo\n",
    "\n",
    "Les colonnes du jeu de données météo sont les suivantes : \n",
    "\n",
    "- WD10M           MERRA-2 Wind Direction at 10 Meters (Degrees) \n",
    "- T2M             MERRA-2 Temperature at 2 Meters (C) \n",
    "- RH2M            MERRA-2 Relative Humidity at 2 Meters (%) \n",
    "- QV2M            MERRA-2 Specific Humidity at 2 Meters (g/kg) \n",
    "- T2MDEW          MERRA-2 Dew/Frost Point at 2 Meters (C) \n",
    "- U10M            MERRA-2 Eastward Wind at 10 Meters (m/s) \n",
    "- PS              MERRA-2 Surface Pressure (kPa) \n",
    "- T2MWET          MERRA-2 Wet Bulb Temperature at 2 Meters (C) \n",
    "- WS10M           MERRA-2 Wind Speed at 10 Meters (m/s) \n",
    "- V10M            MERRA-2 Northward Wind at 10 Meters (m/s) \n",
    "- PRECTOTCORR     MERRA-2 Precipitation Corrected (mm/hour) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser la méthode [drop()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) pour supprimer les colonnes que l'on ne souhaite pas conserver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On supprime les colonnes inutiles\n",
    "## On ne souhaite conserver que les colonnes température, vitesse du vent et pluviométrie.\n",
    "\n",
    "df_weather = *****\n",
    "\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Utiliser la méthode [rename()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) pour renommer les colonnes restantes afin de pouvoir faire une jointure avec les données de disponibilité vélo'v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On renomme les colonnes pour ensuite faire une jointure de ce dataframe avec celui des données vélo'v\n",
    "\n",
    "df_weather = *****\n",
    "\n",
    "df_weather.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On affiche les moyennes des températures, précipitation et vent par mois sur 3 graphiques différents\n",
    "\n",
    "fig,(ax1,ax2,ax3)= plt.subplots(nrows=3)\n",
    "fig.set_size_inches(12,20)\n",
    "\n",
    "monthAggregated = pd.DataFrame(*****).reset_index()\n",
    "monthSorted = monthAggregated.sort_values(by='temperature', ascending = False) \n",
    "sn.barplot(data=monthSorted, x = 'month', y = 'temperature', ax=ax1)\n",
    "ax1.set(xlabel='Month', ylabel='Average Temperature', title='Average Temperature by Month') \n",
    "\n",
    "monthAggregated = pd.DataFrame(*****).reset_index()\n",
    "monthSorted = monthAggregated.sort_values(by='precipitation', ascending = False) \n",
    "sn.barplot(data=monthSorted, x = 'month', y = 'precipitation', ax=ax2)\n",
    "ax2.set(xlabel='Month', ylabel='Average Precipitation', title='Average Precipitation by Month') \n",
    "\n",
    "monthAggregated = pd.DataFrame(*****).reset_index()\n",
    "monthSorted = monthAggregated.sort_values(by='vent', ascending = False) \n",
    "sn.barplot(data=monthSorted, x = 'month', y = 'vent', ax=ax3)\n",
    "ax3.set(xlabel='Month', ylabel='Average Wind Speed', title='Average Wind Speed by Month') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser la méthode [merge()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) pour combiner les données de disponibilité des velo'v avec les données méteo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On ajoute les données météo dans le dataframe qui contient les données de disponibilité\n",
    "df_bikes = *****\n",
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement d'un modèle de prédiction\n",
    "\n",
    "### 5.1 Import de la librairie (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Préparation des données pour l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On fait une copie de notre DataFrame, pour pouvoir revenir aux données initiales si besoin\n",
    "df_data = df_bikes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sépare le jeu de données en entrées et sorties pour l'apprentissage\n",
    "\n",
    "# les inputs correspondent aux données fournis au modèle pour pouvoir apprendre. \n",
    "inputs = *****\n",
    "\n",
    "# les labels correspondent aux valeurs que l'on souhaite prédire et donc que le modèle doit apprendre\n",
    "labels = *****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plupart des méthodes d'apprentissage ne peuvent utiliser que des variables numériques. Il faut donc transformer les variables catégorielles telle que `id_velov` ou `day_of_week` afin qu'elles puissent servir lors de l'apprentissage. \n",
    "\n",
    "Pour cela on utilise la fonction [LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) de la librairie scikit-learn. Elle permet de transformer des valeurs de type chaine de caractère en numérique par association. Par exemple : lundi -> 0, mardi -> 1, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On transforme les variables catégorielles en variables numériques\n",
    "l1_encoder = LabelEncoder()\n",
    "l2_encoder = LabelEncoder()\n",
    "\n",
    "# on entraîne l'encoder pour qu'il ai vu toutes les valeurs possibles\n",
    "l1_encoder.fit(df_stations.id_velov)\n",
    "\n",
    "# on transforme la colonne id_velov\n",
    "inputs['id_velov'] = l1_encoder.transform(inputs['id_velov'])\n",
    "\n",
    "l2_encoder.fit(inputs['day_of_week'])\n",
    "inputs['day_of_week'] = l2_encoder.transform(inputs['day_of_week'])\n",
    "\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que les données sont prêtes et que les variables sont au bon format. Il faut maintenant séparer le jeu de données pour utiliser une partie des données pour l'entraînement et une autre partie pour l'évaluation. \n",
    "\n",
    "Pour cela on utilise la méthode [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) qui permet de séparer le jeu de données en 2 de manière aléatoire pour que les jeux d'entrainement et d'évaluation aient les mêmes caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sépare le jeu de données en 2 : un jeu d'entraînement et un jeu de test\n",
    "x_train, x_test, y_train, y_test = *****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Entraînement supervisé des modèles\n",
    "\n",
    "L'ensemble des algoritmes d'apprentissage supervisée implémentés dans la librairie Scikit-Learn sont disponibles ici : https://scikit-learn.org/stable/supervised_learning.html\n",
    "\n",
    "Dans notre cas, nous souhaitons prédire des valeurs continues par opposition aux valeurs discrètes. Nous allons donc utiliser des modèles de régression et non pas des modèles de classification.\n",
    "\n",
    "Je vous propose de tester et comparer 2 méthodes : [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) et [RandomForestRegressor](https://scikit-learn.org/0.15/modules/generated/sklearn.ensemble.RandomForestRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On déclare un modèle de type RadomForestRegressor et on l'entraîne sur le jeu d'entraînement\n",
    "clf_lr = *****\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise le modèle pour faire la prédiction sur le jeu de test\n",
    "predictions_lr = *****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On déclare un modèle de type RadomForestRegressor et on l'entraîne sur le jeu d'entraînement\n",
    "clf_rf = *****\n",
    "*****\n",
    "\n",
    "# !! Cette cellule peut mettre plusieurs minutes à s'executer (entre 5 et 10 min) !! #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise le modèle pour faire la prédiction sur le jeu de test\n",
    "predictions_rf = *****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Evaluation et comparaison\n",
    "\n",
    "Afin d'évaluer nos modèles de prédiction, on utilise la mesure [MAE](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) (Mean Absolute Error) : \n",
    "$$\\frac{\\sum_{i=1}^n \\left\\lvert \\hat{y}_i - y_i \\right\\rvert}{n}$$ \n",
    "qui permet de mesurer l'écart moyen absolu entre les prédictions et les valeurs à prédire. Plus cette valeur est faible, meilleur est notre modèle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule l'erreur moyenne en comparant les prédictions avec les valeurs qu'il fallait prédire\n",
    "\n",
    "mae_lr = *****\n",
    "\n",
    "print('linear regression mae : ', mae_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule l'erreur moyenne en comparant les prédictions avec les valeurs qu'il fallait prédire\n",
    "\n",
    "mae_rf = *****\n",
    "\n",
    "print('random forest regression mae : ', mae_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que l'erreur moyenne est nettement plus faible pour le modèle random forest ! En moyenne l'écart entre la prédiction et la réalité est de 1. Ce qui parait plutôt satisfaisant pour notre tâche. Dans le cas de l'utilisation de ce modèle dans une application, il faudrait prévenir l'utilisateur que la prédiction est en moyenne correcte à plus ou moins 1 vélo !\n",
    "\n",
    "On souhaite maintenant afficher une évaluation sous forme de graphique. Le jeu de test contenant plus d'un million de lignes, on isole au préalable un échantillon de 100 données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regroupe les valeurs à prédire et les prédictions au sein d'un même tableau\n",
    "t = np.stack((y_test, predictions_rf, predictions_lr), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie la taille du tableau\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche un aperçu des valeurs\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sélectionne un échantillon de 100 prédictions pour afficher la comparaison\n",
    "\n",
    "idx = np.random.randint(t.shape[1], size=100)\n",
    "sample = t[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Comparaison des prédictions\")\n",
    "plt.xlabel(\"Echantillon\")\n",
    "plt.ylabel(\"Vélos disponibles\")\n",
    "plt.plot(range(100), sample[0], color =\"green\", label=\"Vérité\")\n",
    "plt.plot(range(100), sample[1], color =\"blue\", label=\"Random Forest\")\n",
    "plt.plot(range(100), sample[2], color =\"red\", label=\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite maintenant afficher le même type de graphique mais au lieu d'afficher la prédiction on veut afficher l'erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_rf = abs(sample[0]-sample[1])\n",
    "err_lr = abs(sample[0]-sample[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Comparaison des prédictions\")\n",
    "plt.xlabel(\"Echantillon\")\n",
    "plt.ylabel(\"Erreur\")\n",
    "plt.plot(range(100), err_rf, color =\"blue\", label=\"Random Forest\")\n",
    "plt.plot(range(100), err_lr, color =\"red\", label=\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Utilisation du modèle\n",
    "\n",
    "La météo a-t-elle un impact sur la prédiction ?\n",
    "\n",
    "Pour le savoir, nous pouvons faire une prédiction de la disponibilité à une station pour une certaine date en faisant varier la météo et observer s'il y a des différences entre les prédictions.\n",
    "\n",
    "Le modèle prend en entrée 10 paramètres :\n",
    "- id_velov\n",
    "- year\n",
    "- month\n",
    "- day\n",
    "- hour\n",
    "- minute\n",
    "- day_of_week\n",
    "- temperature\n",
    "- vent\n",
    "- precipitation\n",
    "\n",
    "On se propose de faire une prédiction pour samedi prochain à 8h à la station 3094 (Gare Part-Dieu). \n",
    "On fait la prédiction pour 2 cas différents : beau et mauvais temps\n",
    "1. Beau temps : température = 20, vent = 1, précipitations = 0\n",
    "2. Mauvais temps : température = 8, vent = 3, précipitations = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder le nom de la station\n",
    "st = l1_encoder.transform(['velov-3094'])\n",
    "\n",
    "# encoder le jour de la semaine\n",
    "dn = l2_encoder.transform(['Saturday'])\n",
    "\n",
    "d_nice = [[st, 2022, 2, 18, 8, 0, dn, 20, 1, 0]]\n",
    "d_bad = [[st, 2022, 2, 18, 8, 0, dn, 8, 3, 0.10]]\n",
    "\n",
    "# on calcule les prédictions\n",
    "p_nice = *****\n",
    "p_bad = *****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_nice)\n",
    "print(p_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque donc que la météo a bien un impact sur la prédiction. Il y aura plus de vélos disponibles en cas de mauvais temps que de beau temps !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "En reprenant le code des séances précédentes proposer une carte de la disponibilité des vélo'v pour l'ensemble des stations basée sur les prédictions de samedi prochain à 8h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A COMPLETER\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposer une carte permettant de visualiser la différence entre beau et mauvais temps pour la même date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A COMPLETER\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanza-lexicoscope-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n[Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "68d5f9281eab57a7f4901cb150f4c691b1d08935474a18f188e0e3e8f8f412b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
